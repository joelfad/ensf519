{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> ENSF 519.01 Applied Data Scince </center></h1>\n",
    "<h2> <center> Assignment 2: Supervisoed Machine Learning - Classification & Regression (25 marks)</center></h2>\n",
    "<h2> <center> Due: . Feb 16, 2018. To be submitted on D2L Dropbox </center></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignemnt focuses on applying basic classfication and regression techniques in the context of sofware quality and performance, in four parts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part A. Defect Prediction: Binary Classification (7 marks)</h2>\n",
    "<br>\n",
    "\n",
    "Software quality assurance (SQA) techniques (e.g., testing, code review, etc.) are among the major tasks in software development that try to eliminate software defects, as much as possible, prior to deployment. However, they are also expensive and time consuming. Therefore, it is very beneficial to narrow the scope of SQA to the parts of the software (e.g., files, classes, or even methods) that are defective. Obviously, before applying SQA, the defective parts are unknown, therefore, “software defect prediction” techniques try to use machine learning to predict which parts of the software (e.g., which classes) are more likely to be defective. Thus the SQA effort can be proportionally allocated to them (more SQA resources for parts that are predicted to be defective). \n",
    "\n",
    "In Parts A, B, and C, you will apply several supervised machine learning techniques to a defect dataset from a software repository, which is belong to a NASA system. The dataset have collected a set of software related metrics from the history of the software, per module. They also recorded the existence of a defect per module. In other words, your feature set is the metrics array and the targets are the existence of defects. \n",
    "\n",
    "The goal is to build a model to predict the target based on the features from the historical data. \n",
    "\n",
    "To read more about the dataset and the features collected see:\n",
    "\n",
    "NASA dataset:http://openscience.us/repo/defect/mccabehalsted/jm1.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you should read the NASA data that is given to you as a .csv file ( NasaData.csv ). Use pandas read_csv function for this.\n",
    "\n",
    "After reading the data, build binary classification models with LogisticRegression, LinearSVC, GaussianNB, and KNeighborsClassifier, from SKLearn. Each model gets metrics as feature set and predicts either a defective (1) or not defective (0) label.\n",
    "\n",
    "Take 75% of data as training set and 25% of it as test set. To eliminate the randomness when splitting the data, you should run each classification technique 30 times with seeds from [1 to 30]. \n",
    "\n",
    "Using model_selection's train_test_split function, randomly select 1/4 of your dataset as training and 3/4 as test set. \n",
    "Calculate accuracies per technique and repeat this for a total of 30 random runs (every run will use a different random seed in train_test_split and return a separate accuracy value per model).\n",
    "\n",
    "Visualize the distribution of the accuracies for each model in a single box plot. All boxplots should be located in one figure, where The X_axis is the four classification techniques that you've applied and the Y_axis is the accuracies.\n",
    "\n",
    "Identify which model (with default parameters) has the lowest/most unreliable results.\n",
    "\n",
    "Improve that model to be almost in par with others, using regularization.\n",
    "\n",
    "*** Note1 you must use pandas for both reading from CSV and visualizing boxplots ***\n",
    "<br>\n",
    "\n",
    "*** Note2 only report final results. That is after regularization for the worst technique, together with defaults for other techniques ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer:</b> ..................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part B. Model complexity (6 marks)</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will focus only on the kNN algorithm and try to tune it's complexity.\n",
    "\n",
    "<h2>Part B.1. KNN complexity tuning (3 marks)</h2>\n",
    "In this part, we want to tune the value k in kNN for our NASA dataset. To do so, you should find a sweet spot that the model is neither overfitted nor underfitted. \n",
    "Here again take the NASA dataset and apply the model_selection's train_test_split with 75% training and 25% test data, but with a fix random_state=42.  \n",
    "\n",
    "Then build a K-Nearest-Neighbors model using k=1,3,5,..,49. Finally, plot the accuracy of your models on the training dataset and the testing dataset, using two lines in one plot.  \n",
    "\n",
    "Using this plot identify what the best value is for k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part B.2. KNN weight parameter (3 marks)</h2>\n",
    "\n",
    "Report your Part B.1. experiment but with KNN's 'weights' parameter set to 'distance'.\n",
    "\n",
    "Visualize the new results. \n",
    "\n",
    "Explain what has happend and why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part C. Combining Models (8 marks)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to try to maximize our defect prediction accuracy on NASA dataset, by combining three models in Part A: (Logistic Regression, Gaussian Naive Bayes, and K-Nearest Neighbour) into one prediction.\n",
    "\n",
    "To do so, you need to implement a function, called Combine, that returns the weighted average votes, for any three models that it receives. \n",
    "\n",
    "Combine works as follows: \n",
    "\n",
    "1) Each model is trained individually \n",
    "\n",
    "2) Each model predicts the test set data points and calculates estimation probabilities as given by the models\n",
    "\n",
    "3) The final score per data point in the test set is calculated by the following formula:\n",
    "\n",
    "score=Sum(predict_i*probability_i)/3)\n",
    "\n",
    "where predict_i is the prediction by model_i (predictions are either -1 OR +1 for the two binary classes) and probability_i is the estimation probability (predict_proba) given by model_i.\n",
    "\n",
    "4) The weighted average vote is ZERO if the score is less than or equal to zero otherwise the weighted average vote is ONE\n",
    "\n",
    "\n",
    "Finally, calculate the accuracy of the combined model on the test data as follows:\n",
    "\n",
    "    Accuracy= number_of_correct_prediction / total_number_of_predictions \n",
    "    \n",
    "All three models should be called by their default parameters as in Part A.\n",
    "    \n",
    "The same as Part A run the Combine model 30 times (with different random seeds) and visualize the results beside the individual models' results, from Part A with a pandas boxplot.\n",
    "\n",
    "<font color='red'>Note1: Use numpy and Pandas as much as possible to increase performance </font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part D. Regression (4 marks)</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use a new data set which is related to the performance of several CPUs. These CPUs are of different specifications, and you have the estimated relative performance(ERP) metric per CPU, in this data set.\n",
    "\n",
    "columns of data set are as follows:\n",
    "\n",
    "    MYCT: machine cycle time in nanoseconds (integer) \n",
    "    MMIN: minimum main memory in kilobytes (integer) \n",
    "    MMAX: maximum main memory in kilobytes (integer) \n",
    "    CACH: cache memory in kilobytes (integer) \n",
    "    CHMIN: minimum channels in units (integer) \n",
    "    CHMAX: maximum channels in units (integer) \n",
    "    PRP: published relative performance (integer) \n",
    "    ERP: estimated relative performance from the original article (integer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part D.1. Ridge (2 marks)</h2>\n",
    "<br>\n",
    "Read the data that is given to you as a CSV file (\"CPU_Performance.csv\") and take 75% of it as training set and 25% of it as test set with random_state=42. Use default Linear Regression and Linear Regression with Ridge regularization, separately, to predict ERP using the other columns as features. Print the score of these two models on training set and test set data. \n",
    "\n",
    "Use the default setup for the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part D.2. Lasso (2 marks)</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, first tune your Ridge model from PartD.1, using the regularization parameter, to improve R^2. \n",
    "\n",
    "Then use Linear Regression with Lasso regularization on the same train and test set. Tune Lasso to minimize the number of selected features but still achieve higher R^2 than the basic Linear regression results in Part D.1.\n",
    "\n",
    "Report the R^2 and the selected features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer</b>: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
