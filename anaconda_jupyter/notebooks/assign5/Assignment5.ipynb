{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> ENSF 519.01 Applied Data Scince </center></h1>\n",
    "<h2> <center> Assignment 5: Bug Localization (25 marks)</center></h2>\n",
    "<h2> <center> Due: April 13, 4pm . To be submitted on D2L Dropbox </center></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we are going to provide an automated system that given a bug report and the history of bug reports, suggests what files are likely to be buggy and need to be fixed. This problem in the software engineering domain is called Fault Localization and there are several solutions for it. Our approach will be based on TF.IDF and topic vector representations of the bug reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step1: Preprocessing: Tokenizing - removing stopwords - stemming - vectorizing (7 marks)</h2>\n",
    "<br><br>\n",
    "\n",
    "Our dataset consists of a potion of Eclipse project's bug reports collected from. \n",
    "\n",
    "Link to Data: https://zenodo.org/record/268425#.WrVqNdPOVTZ\n",
    "\n",
    "For each bug report the dataset have the following items: \n",
    "\n",
    "<ul>\n",
    "<li> id: The ID which is assigned to each bug report starting from 1.\n",
    "<li> bug_id: The bug report ID in the Eclipse company. \n",
    "<li> summary: A small description of bug in one line\n",
    "<li> report_time: The date and time of the arrival of the bug report\n",
    "<li> report_timestamp: A timeStamp assigned to each bug report when they arrive\n",
    "<li> status: shows the status of bug at the time being. \n",
    "<li> commit: commit number.\n",
    "<li> commit_timestamp: the commit time.\n",
    "<li> files: files that have been modified to fix the bug\n",
    "<li> description: The actual text of bug report. \n",
    "</ul>\n",
    "\n",
    "We have created a shortened and cleaned version of the reports containing 100 bug reports for you with the following features:\n",
    "\n",
    "<ul>\n",
    "<li> Summary\n",
    "<li> Description \n",
    "<li> Files\n",
    "</ul>\n",
    "\n",
    "All these bugs are \"Closed\" bug repots (they are already fixed)\n",
    "\n",
    "Steps: \n",
    "<ol>\n",
    "<li> Read the data from Eclipse_Platform_UI_100.csv \n",
    "<li> For each bug report in the dataset, take the Description + Summary as the document\n",
    "<li> For each Document in the corpus, also keep a list of Files modified to fix the bug in a list, per bug. \n",
    "<li> Tokenize the Documents \n",
    "<li> Remove the stopwords from the Documents (Use SKLearn's stopword list)\n",
    "<li> Stem the remaining using PorterStemmer (from nltk). \n",
    "<li> Finally, vectorize the corpus documents using CountVectorizer.\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 2: LDA Model (10 marks)</h2>\n",
    "<br><br>\n",
    "In this step, we will apply an LDA model on the corpus documents and represent them using topic membership vectors. We then use this vectors to measure similarity between a document from the test set (query) and the train set documents. Then we score the train set documents according to their similarities to the query. Finally, we identify the files relevant to the query based on the modified files from the similar bug reports.\n",
    "\n",
    "Since the documents have timestapms (bugs come and get fixed one after the other), to evaluate our approach, the best way is to use a time series cross-validation technique from SKLearn, but for the sake of simplicity we only take one train_test_split (20 vs. 80).\n",
    "\n",
    "Steps to follow: \n",
    "<ol>\n",
    "<li> Divide the corpus into a test and train set as follows: take the first 20 documents as test set and the rest as the training set.\n",
    "<li> Using SKLearn, build an LDA model using the Bag of Words vectors of the documents, created in the previous question (with number of topics = 100). - Note that we give the entire train and test to LDA to build a model. This is not information leaking since we are doing unsupervised decomposition here and the LDA does not uses any information that is from future.\n",
    "<li> For each document in both train and the test sets, find the topic membership vector (Vector of Probability). Vector of Probability is a vector that indicates the probability of the document to be a member of each topic. Now to compare two documents, we need to compare their Vectors of Probability. \n",
    "<li> For each test set document (query document):\n",
    "  <ol>\n",
    "  <li>apply \"Hellinger Distance\" (which is a suitable distance function for Vectors with values between 0 and 1.0) to calculate it's distance to every train set document. You can import hellinger from gensim.matutils </li>\n",
    "  <li>assign the inverse of Hellinger Distance per training set document as its \"Similarity Score\"</li>\n",
    "  <li>For each file, from the list of files modified in any bug fix in the training set (provided as the \"files\" attribute per bug report), calculate a Score, by summing Similarity Scores of all the training set documents, in which the file is seen. </li>\n",
    "  <li>rank files based on the calculated Scores. </li>\n",
    "  <li>pick a cut-off Threshold and form a predicted list of files (with Threshold = 5, 10, 15, 20, 25) </li>\n",
    "  <li>Calculate Precision and Recall by comparing the predicted list with the actual list of modified files per each bug report in the test set. </li>\n",
    "</ol>\n",
    "<li> Report median Precision and Recall over all 20 test bug report\n",
    "<li> Plot the Precision and Recall for different values of threshold\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question:</b> Which evaluation metric (Recall or Precision) is more important in this context? What is the easiest way to improve that measure? What is the practical problem with that way of improvement?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer:</b> ..................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 3: TF.IDF rescaling (8 marks) </h2>\n",
    "<br>In this part, we replicate the previous part but this time using a new distance function that we directly apply on TF.IDF-based representation of vectorized documents and NOT the topic memberships.  \n",
    "\n",
    "Steps to follow: \n",
    "<ol>\n",
    "<li> Using TF.IDF covert the vectorized documents from Step 1 to a matrix of doc-terms, where the cell value at (i,j) is the frequency of term j in doc i, multiplied by the idf of term j across the corpus.\n",
    "<li> Now to compare two documents (two rows in the matrix) you can use Cosine Similarity measure. Since SKLearn already normalizes the TF.IDF values so that each document has a length of UNIT, you can simply take the Numpy's \"Inner Product\" of two rows (arrays) as their Cosine Similarity score. See https://en.wikipedia.org/wiki/Cosine_similarity for more details on the Cosine Similarity function. \n",
    "<li> After you have the Similarity Scores the rest is exactly like previous Step, at 4.C. So you basically will output the similar Precision-Recall plot.\n",
    "</ol>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question: </b> Given that both LDA and TF.IDF provide low quality results, explian what can be the main reason? \n",
    "Note that you do not need to implment anything extra here. <br>\n",
    "This is an open question with no definite answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Your Answer:</b> ..................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
