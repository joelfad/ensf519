{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> ENSF 519.01 Applied Data Scince </center></h1>\n",
    "<h2> <center> Assignment 4: Unsupervised learning (25 marks)</center></h2>\n",
    "<h2> <center> Due: Macrh 25, Midnight . To be submitted on D2L Dropbox </center></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment the focus is on using unsupervised learning e.g., clustering and data transformation to get better understanding of the data (exploratory analysis) or predictions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part A. Local vs. Global Prediction (7 marks)</h2>\n",
    "<br><br>\n",
    "In many situations, your training dataset is very large to include as many observations as possible, which is typically a good thing. For instance, a complex prediction model for automated image captioning works best if the learning dataset is massive and rich. However, if you are using a simple model (like a linear regression) for any reason (e.g., speed, interpretability, etc.) one caveat of very large datasets is that the learned models might actually become very far off from your test set.\n",
    "\n",
    "For instance, assume you have to predict online sales for a particular book. But your training set is the entire Amazonâ€™s historical sales records. Obviously, a simple linear model will not work well, trying to fit a line that predicts sales of everything, from books, to grocery, to toys, etc.\n",
    "\n",
    "One simple solution could be trying to train your model only on a portion of the training set that is closer to your interested data. For instance, in the above example, only train on the book records. \n",
    "\n",
    "In this part of HW4, we want to evaluate this idea automatically, by first clustering the training dataset and then fitting the model only on the closets cluster to the test data.\n",
    "\n",
    "\n",
    "So you need to follow these steps: \n",
    "\n",
    "- Read data from OO-DefectPrediction.csv dataset (used in HW3 for defect prediction)\n",
    "- Take 90% of data as train and 10% as test using train_test_split with random_state=0. \n",
    "- Cluster the training set to multiple clusters using Kmeans. (K from 2 to 5, inclusive)\n",
    "- Find the most similar cluster to the test set. To do this predict the data set items and see which cluster they fall in. Then use the majority vote to decide which cluster is closer to the test data (basically the one that is predicted most, as label, for the test data items)\n",
    "- Build a global prediction model the same as the one you built in HW 3 PartB (using original entire train set)\n",
    "- Build a local linear regression model where you use only the closets cluster as your train dataset (all other setups unchanged). \n",
    "- Compare the result (r2_score) between the global and the local model for each K and explain your observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Part A. Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question: Explain your observation in terms of R^2 of global and local models across different Ks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer:</b> ..................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part B. Analyze Clustering Algorithms (8 marks) </h2>\n",
    "In this part, you will excerise the same type of cluster analysis we tried in class, but this time on a different dataset (Movie Dataset)\n",
    "\n",
    "Preprocessing Steps (2 mark):\n",
    "- Read data from \"movie_metadata.csv\" \n",
    "- Remove duplicate movies (same title), if any exists\n",
    "- Keep only these columns ['director_name', 'actor_1_name', 'budget', 'imdb_score'] as features\n",
    "- Remove any row that has an empty feature, among those we have kept\n",
    "- Use one-hot-encoding in Pandas to transfer categorical data\n",
    "- Finally, scale both 'budget' and 'imdb_score' features using MinMaxScaler \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Part B. Preprocessing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part B.1 Tune DBSCAN (3 marks) </h3>\n",
    "\n",
    "Now apply a DBSCAN clustering algorithm on the preprocessed data.\n",
    "- First tune DBSCAN using eps : [1,2,3] and min_sample: [3, 5, 7].\n",
    "- For each setup, run DBSCAN twice. Once on the scaled data and once with the original 'budget' and 'imdb_score' values.\n",
    "- print number of clusters, cluster sizes, and number of noises. For both cases, per setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Part B.1 DBSCAN \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain your observations in terms of eps and min_sample effect on the scaled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer:</b> ..................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain your observations in terms of scaling effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer:</b> ..................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part B.2 KMeans and Agglomerative Clustering  (3 marks) </h3>\n",
    "In this section, use the scaled dataset and first draw a dendrogram using ward function in scipy. Explian why K= 10 seems like a reasonable choice.\n",
    "Comapre ARI of KMeans and agglomerative using K=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Part B.3 KMeans and Agglomerative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part C. Feature Selection (6 marks)</h1>\n",
    "<br>\n",
    "\n",
    "In this section, we are going to select the most informative features from the NASA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Part C.1. Feature selection using ANOVA (3 marks)</h2>\n",
    "\n",
    "Steps:\n",
    "    \n",
    "- Read data from the NASA.csv\n",
    "- Using ANOVA select top K features, whith K=[1..10]\n",
    "- Build LogisticRegression models, one with the original data, and one for each K (using a subset of feature)\n",
    "- Compare the accuracies and find the best K (based on the median of 30 runs with random_state=[0 to 30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Part C.1 ANOVA \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part C.2. Compare feature selection models (3 marks)</h2>\n",
    "\n",
    "Now apply SelectFromModel and RFE and compare them with SelectKBest, as follows:\n",
    "\n",
    "- Apply the three techniques so that you reduce the features to only 6 features (note that 6 is not necessarily the best K from Part C.1)\n",
    "- Report the prediction scores of a LogisticRegression model on the selected features of each model.\n",
    "- Print the name of features selected by each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Part C.2 Compare feature selection models \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part D. Data Tranformation (4 marks)</h1>\n",
    "\n",
    "In this part, you are going to work with a new data set which contains some the features of a house collected over time. \n",
    "The objective of this part is to help improve linear model's predicitons using data transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part D.1 Binning (2 marks)</h2>\n",
    "\n",
    "Our first try is using binning, as follows:\n",
    "\n",
    "- Read from MyHouse.csv (take 'Light' as the data target and the rest of the columns as data features ) \n",
    "- First apply a LinearRegression on the original data to predict the target and report the score of the model on the test set. \n",
    "- Now apply binning on all three columns \n",
    " (for Temperature make 5 Bins -- for Humidity make 10 bins -- and for CO2Bins make 11 bins)\n",
    "- Print your data shape before and after binning.\n",
    "- Now again apply LinearRegression on the new data and report the score again. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Part D.1 Compare feature selection models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part D.2 Polynomials (2 marks)</h2>\n",
    "\n",
    "To compare polynomials and binning, apply polynomials on all three features. \n",
    "- Use degree=6.\n",
    "- Print your data shape before and after transformation\n",
    "- Apply LinearRegression on the new data and report the score again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Part D.2 Compare feature selection models \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
