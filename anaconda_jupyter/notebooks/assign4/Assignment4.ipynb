{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> ENSF 519.01 Applied Data Scince </center></h1>\n",
    "<h2> <center> Assignment 4: Unsupervised learning (25 marks)</center></h2>\n",
    "<h2> <center> Due: March 25, Midnight . To be submitted on D2L Dropbox </center></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment the focus is on using unsupervised learning e.g., clustering and data transformation to get better understanding of the data (exploratory analysis) or predictions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part A. Local vs. Global Prediction (7 marks)</h2>\n",
    "<br><br>\n",
    "In many situations, your training dataset is very large to include as many observations as possible, which is typically a good thing. For instance, a complex prediction model for automated image captioning works best if the learning dataset is massive and rich. However, if you are using a simple model (like a linear regression) for any reason (e.g., speed, interpretability, etc.) one caveat of very large datasets is that the learned models might actually become very far off from your test set.\n",
    "\n",
    "For instance, assume you have to predict online sales for a particular book. But your training set is the entire Amazonâ€™s historical sales records. Obviously, a simple linear model will not work well, trying to fit a line that predicts sales of everything, from books, to grocery, to toys, etc.\n",
    "\n",
    "One simple solution could be trying to train your model only on a portion of the training set that is closer to your interested data. For instance, in the above example, only train on the book records. \n",
    "\n",
    "In this part of HW4, we want to evaluate this idea automatically, by first clustering the training dataset and then fitting the model only on the closets cluster to the test data.\n",
    "\n",
    "\n",
    "So you need to follow these steps: \n",
    "\n",
    "- Read data from OO-DefectPrediction.csv dataset (used in HW3 for defect prediction)\n",
    "- Take 90% of data as train and 10% as test using train_test_split with random_state=0. \n",
    "- Cluster the training set to multiple clusters using Kmeans. (K from 2 to 5, inclusive)\n",
    "- Find the most similar cluster to the test set. To do this predict the data set items and see which cluster they fall in. Then use the majority vote to decide which cluster is closer to the test data (basically the one that is predicted most, as label, for the test data items)\n",
    "- Build a global prediction model the same as the one you built in HW 3 PartB (using original entire train set)\n",
    "- Build a local linear regression model where you use only the closets cluster as your train dataset (all other setups unchanged). \n",
    "- Compare the result (r2_score) between the global and the local model for each K and explain your observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "Cluster memberships:\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "---\n",
      "Predictions:\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0]\n",
      "###\n",
      "Cluster memberships:\n",
      "[0 0 0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 2\n",
      " 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 2 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0\n",
      " 0 2 0 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 2 0\n",
      " 0 2 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "---\n",
      "Predictions:\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0]\n",
      "###\n",
      "Cluster memberships:\n",
      "[0 3 0 0 0 0 1 0 0 2 3 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 3 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 3 0 0 3 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 3 0 0 0 0 0 0 3 0 0 0 3 0 0 3\n",
      " 0 0 3 0 3 0 0 0 0 0 0 0 0 0 3 0 0 0 0 3 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 3 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 3 1 0 0 0 0 0 0 0 3 0 0 2 1 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 3 0 1 0 0 0 0 0 3 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3 0 0 0 0 0 0 0 0 1 0\n",
      " 0 3 0 0 0 0 0 0 1 1 0 0 0 0 3 0 0 1 0 3 0 0 0 0 0 0 0 0 0 3 3 0 0 0 0 3 0\n",
      " 1 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 3 0 0 0 0 0 3 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "---\n",
      "Predictions:\n",
      "[0 3 0 0 0 0 0 2 0 0 0 0 0 1 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 3 0 2 3 0 0 0\n",
      " 0 0 0 0 0 2 0 0 0]\n",
      "###\n",
      "Cluster memberships:\n",
      "[4 0 4 4 4 4 2 4 4 1 0 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 4 0 4 2 4 4 4 4 4 4\n",
      " 4 4 4 4 4 0 4 4 0 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 2 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 0 4 4 4 4 4 4 2 4 4 4 0 4 4 0\n",
      " 4 4 0 4 0 4 4 4 4 4 4 4 4 4 0 4 4 4 4 0 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 0 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 0 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 0 2 4 4 4 4 4 4 4 0 4 4 3 2 4 4 4 4 4 4 4 4 3 4 4 2 4 4 0 4 4 4 4 4 4 4\n",
      " 4 0 4 2 4 4 4 4 4 0 4 4 4 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 0 4 4 4 4 4 4 4 4 2 4\n",
      " 4 0 4 4 4 4 4 4 2 2 4 4 4 4 0 4 4 2 4 0 4 4 4 4 4 4 4 4 4 0 0 4 4 4 4 0 4\n",
      " 2 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 2 4 4 0 4 4 4 4 4 0 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4 4 4 4 4 4 0]\n",
      "---\n",
      "Predictions:\n",
      "[4 0 4 4 4 4 4 3 4 4 4 4 4 2 4 4 4 0 4 4 4 4 4 4 4 4 4 4 4 4 0 4 3 0 4 4 4\n",
      " 4 4 4 4 4 3 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "# pandas v0.22.0\n",
    "import pandas as pd\n",
    "\n",
    "# numpy v1.13.3\n",
    "import numpy as np\n",
    "\n",
    "# sklearn v0.19.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# matplotlib v2.1.2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#\n",
    "# Read data from OO-DefectPrediction.csv dataset (used in HW3 for defect prediction)\n",
    "#\n",
    "defect_path = \"OO-DefectPrediction.csv\"\n",
    "try:\n",
    "    defect_df = pd.read_csv(defect_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "#\n",
    "# Take 90% of data as train and 10% as test using train_test_split with random_state=0. \n",
    "#\n",
    "target_col = \"bug\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    defect_df.drop(target_col, axis=1),\n",
    "    defect_df[target_col],\n",
    "    test_size=0.10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# display(defect_df)\n",
    "\n",
    "\n",
    "#\n",
    "# Cluster the training set to multiple clusters using Kmeans. (K from 2 to 5, inclusive)\n",
    "#\n",
    "kmeans = dict()\n",
    "for i in range(2, 6):\n",
    "    kmeans[i] = KMeans(n_clusters=i)\n",
    "    kmeans[i].fit(X_train)\n",
    "    print(\"###\")\n",
    "    print(\"Cluster memberships:\\n{}\".format(kmeans[i].labels_))\n",
    "    print(\"---\")\n",
    "    print(\"Predictions:\\n{}\".format(kmeans[i].predict(X_test)))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find the most similar cluster to the test set. To do this predict the data set items and\n",
    "# see which cluster they fall in. Then use the majority vote to decide which cluster is closer\n",
    "# to the test data (basically the one that is predicted most, as label, for the test data items)\n",
    "\n",
    "\n",
    "\n",
    "# Build a global prediction model the same as the one you built in HW 3 PartB (using original entire train set)\n",
    "\n",
    "\n",
    "\n",
    "# Build a local linear regression model where you use only the closest cluster as your train dataset\n",
    "# (all other setups unchanged).\n",
    "\n",
    "\n",
    "\n",
    "# Compare the result (r2_score) between the global and the local model for each K and explain your observations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question: Explain your observation in terms of R^2 of global and local models across different Ks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer:</b> ..................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part B. Analyze Clustering Algorithms (8 marks) </h2>\n",
    "In this part, you will excerise the same type of cluster analysis we tried in class, but this time on a different dataset (Movie Dataset)\n",
    "\n",
    "Preprocessing Steps (2 mark):\n",
    "- Read data from \"movie_metadata.csv\" \n",
    "- Remove duplicate movies (same title), if any exists\n",
    "- Keep only these columns ['director_name', 'actor_1_name', 'budget', 'imdb_score'] as features\n",
    "- Remove any row that has an empty feature, among those we have kept\n",
    "- Use one-hot-encoding in Pandas to transfer categorical data\n",
    "- Finally, scale both 'budget' and 'imdb_score' features using MinMaxScaler \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from copy import copy\n",
    "\n",
    "#\n",
    "# Read data from \"movie_metadata.csv\"\n",
    "#\n",
    "movie_path = \"movie_metadata.csv\"\n",
    "try:\n",
    "    movie_df = pd.read_csv(movie_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "#\n",
    "# Remove duplicate movies (same title), if any exists\n",
    "#\n",
    "movie_df = movie_df.drop_duplicates(\"movie_title\")\n",
    "\n",
    "\n",
    "#\n",
    "# Keep only these columns ['director_name', 'actor_1_name', 'budget', 'imdb_score'] as features\n",
    "#\n",
    "features_to_keep = ['director_name', 'actor_1_name', 'budget', 'imdb_score']\n",
    "movie_df = movie_df[features_to_keep]\n",
    "\n",
    "\n",
    "#\n",
    "# Remove any row that has an empty feature, among those we have kept\n",
    "#\n",
    "movie_df = movie_df.dropna()\n",
    "\n",
    "\n",
    "#\n",
    "# Use one-hot-encoding in Pandas to transfer categorical data\n",
    "#\n",
    "movie_df = pd.get_dummies(movie_df)\n",
    "\n",
    "\n",
    "#\n",
    "# Finally, scale both 'budget' and 'imdb_score' features using MinMaxScaler\n",
    "#\n",
    "features_to_scale = ['budget', 'imdb_score']\n",
    "scaler = MinMaxScaler()\n",
    "movie_df_scaled = copy(movie_df)\n",
    "movie_df_scaled[features_to_scale] = scaler.fit_transform(movie_df_scaled[features_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part B.1 Tune DBSCAN (3 marks) </h3>\n",
    "\n",
    "Now apply a DBSCAN clustering algorithm on the preprocessed data.\n",
    "- First tune DBSCAN using eps : [1,2,3] and min_sample: [3, 5, 7].\n",
    "- For each setup, run DBSCAN twice. Once on the scaled data and once with the original 'budget' and 'imdb_score' values.\n",
    "- print number of clusters, cluster sizes, and number of noises. For both cases, per setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## eps: 1, min_samples: 3 ##\n",
      "-- original --\n",
      "Clusters: 1, Noise: 4416, Points per cluster:\n",
      "[3]\n",
      "-- scaled --\n",
      "Clusters: 42, Noise: 4255, Points per cluster:\n",
      "[10  6  3  3  4 10  3  3  3  4  4  3  7  3  3  4  3  5  4  5  4  3  3  4  3\n",
      "  3  4  4  3  3  4  3  3  5  3  4  3  3  3  3  3  3]\n",
      "\n",
      "## eps: 1, min_samples: 5 ##\n",
      "-- original --\n",
      "Clusters: 0, Noise: 4419, Points per cluster:\n",
      "[]\n",
      "-- scaled --\n",
      "Clusters: 7, Noise: 4371, Points per cluster:\n",
      "[10  6 10  7  5  5  5]\n",
      "\n",
      "## eps: 1, min_samples: 7 ##\n",
      "-- original --\n",
      "Clusters: 0, Noise: 4419, Points per cluster:\n",
      "[]\n",
      "-- scaled --\n",
      "Clusters: 3, Noise: 4392, Points per cluster:\n",
      "[10 10  7]\n",
      "\n",
      "## eps: 2, min_samples: 3 ##\n",
      "-- original --\n",
      "Clusters: 318, Noise: 2366, Points per cluster:\n",
      "[ 88  50  56   3   9  10   5   4   7  65 104   4   4   3   4  39   3   9\n",
      "   3   5   5  24   7   4   6   4   4  88   4   3   6   8   6  59   4   3\n",
      "   4   3   5   5   5   4   4   4   5   4   3   4   3   6   3   4   4   3\n",
      "   6   6   3  13   3  13   3   3   4   6  10   9   5   3   4   6   3   4\n",
      "   5   4  32  15   7  42   5  11   4   6   3  12   5   4   4   4   3   3\n",
      "   3   3   3   4  26   5   5   3  11   9   3  31   3   3   5   3   3  10\n",
      "   3   3   3   4   3   4   4   8   3   6   5   4   4  32   4   4   5   3\n",
      "   3   4   4   4   3   5   4   6   3   3   6   5   3   3   3   4   9   3\n",
      "   3  10   7   3   3   6   3   4   8   6   4   4   3   5   3   6   3   5\n",
      "   3   3   4   4   4  13   3   3   3   3   5   3   3   3   3   6   3   3\n",
      "   3   5   4   3   3   3   3   3   3   4   3   4   8   7   3   6   6   4\n",
      "   3   5   3   4   3   3   5   4   5   4   4   4   3   3   3   4   3   4\n",
      "   3   3   4   7   6   3   5   5   5   3   3   4   3   4   3   4   4   3\n",
      "   3   6   3   4   3   5   4   3   3   3   3   3   3   8   7   4   4   4\n",
      "   3   6   3   4   9   3   3   3   3   4   3   8   5   5   3   3   3   4\n",
      "   6   3   3   3   3   4   3   3   4   3   6   5   3   3   3   3   3   5\n",
      "   3   3   7   4   4   4   3   3   3   3   4   3   3   3   3   3   3   3\n",
      "   5   4   3   3   3   3   3   3   3   3   3   3]\n",
      "-- scaled --\n",
      "Clusters: 6, Noise: 389, Points per cluster:\n",
      "[4014    3    4    3    3    3]\n",
      "\n",
      "## eps: 2, min_samples: 5 ##\n",
      "-- original --\n",
      "Clusters: 86, Noise: 3303, Points per cluster:\n",
      "[70 42 55  9  6  5  7 96 64 39  9  5 22  6 59  6  6  6  5  5  5  6 16 50  5\n",
      " 31 14 20  6 12 22  5  5 11 22 11  5 11  8  8  6 29  5  5 10 10  9  5  6  5\n",
      "  5  8 10  9  5  5 10  6  9  5  6  6  7  5  5  7  6  5  5 16  5  5  6  7  9\n",
      "  7  6  5  5  5  5  6  5  6  4  5]\n",
      "-- scaled --\n",
      "Clusters: 1, Noise: 503, Points per cluster:\n",
      "[3916]\n",
      "\n",
      "## eps: 2, min_samples: 7 ##\n",
      "-- original --\n",
      "Clusters: 39, Noise: 3805, Points per cluster:\n",
      "[32 45  7 84 29 21 21 31 16 16  9  7 18  7 12 10  9  8  7 26 24  9  8 24 10\n",
      " 15 11  7 14  9 11  7  7  7  7  7  8  7  7]\n",
      "-- scaled --\n",
      "Clusters: 1, Noise: 632, Points per cluster:\n",
      "[3787]\n",
      "\n",
      "## eps: 3, min_samples: 3 ##\n",
      "-- original --\n",
      "Clusters: 176, Noise: 325, Points per cluster:\n",
      "[136 117 129  53  38 139  67  64  75  21  91  45 138  45  21  77  69  45\n",
      " 168  15  99  17  14   4   6  37   6 107  34   6  13  41  49  55  27  79\n",
      "  19  51  10  41  19  41   3  13 134  40  17   8  90  59   3   9  20  73\n",
      "   8  25  15  17   7  21  47  50  25   5  45  11   3  60  11   6   6  16\n",
      "   5  56   6   4   3   9   6   4   8  22  31   3   3  18   5   3  25  41\n",
      "  15   6   7  27  10  15  28   7   4   5   8  17   7  54  12   3   6  12\n",
      "   5   5   6   5   5  21   3   9   8   7  19   6   4   5   4   4   3   3\n",
      "   7  30   9   3   5  10   8  12   9   3  12   5   7   5   4   3   5   6\n",
      "   8   4  12   5   8   5   3   4   4   4   5   4   3   5  10   6   7   3\n",
      "   8   4   3   5   5   6   5   5   3   3   3   3   3   3]\n",
      "-- scaled --\n",
      "Clusters: 1, Noise: 0, Points per cluster:\n",
      "[4419]\n",
      "\n",
      "## eps: 3, min_samples: 5 ##\n",
      "-- original --\n",
      "Clusters: 138, Noise: 456, Points per cluster:\n",
      "[136 117 129  53  38 139  67  64  75  21  91  45 138  45  21  77  69  45\n",
      " 168  15  99  17  14   6  37   6 107  34   6  13  41  49  55  27  79  19\n",
      "  51  10  41  19  41  13 134  40  17   8  90  59   9  20  73   8  25  15\n",
      "  17   7  21  47  50  25  45  11  60  11   6   6  16   5  56   6   9   8\n",
      "  22  31  18   5   5  25  41  15   6   7  27  10  15  28   7   5   8  17\n",
      "   7  54  12   5   5   5   6   5   5  21   8   8   7  19   6   7  30   9\n",
      "   5  10   8  12   9  12  12   5   7   5   6   8  12   5   8   5   5   5\n",
      "  10   6   7   8   5   5   5   6   5   5   5   5]\n",
      "-- scaled --\n",
      "Clusters: 1, Noise: 0, Points per cluster:\n",
      "[4419]\n",
      "\n",
      "## eps: 3, min_samples: 7 ##\n",
      "-- original --\n",
      "Clusters: 102, Noise: 653, Points per cluster:\n",
      "[136 117 129  53  38 139  67  64  75  21  91  45 138  45  21  77  69  45\n",
      " 168  15  99  17  14  37 107  34  13  41  49  55  27  79  19  51  10  41\n",
      "  19  41  13 134  40  17  90  59   9  20  73  25  15  17   7  21  47  50\n",
      "  25  45  11  60  11  16  56   9   8  22  31  18  25  41  15   8   7  27\n",
      "  10  15  28  17   7  54  12  21   7   8  19   8   7  30   9  10   8  12\n",
      "   9  12  12  12   8   7  10   7   7   7   7   8]\n",
      "-- scaled --\n",
      "Clusters: 1, Noise: 0, Points per cluster:\n",
      "[4419]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#\n",
    "# First tune DBSCAN using eps: [1,2,3] and min_sample: [3, 5, 7].\n",
    "#\n",
    "for e in range(1, 4):\n",
    "    for s in range(3, 9, 2):\n",
    "        dbscan = DBSCAN(eps=e, min_samples=s)\n",
    "\n",
    "        # original values\n",
    "        labels = dbscan.fit_predict(movie_df)\n",
    "        bincounts = np.bincount(labels + 1)\n",
    "        print(\"## eps: {}, min_samples: {} ##\\n-- original --\".format(e, s))\n",
    "        print(\"Clusters: {}, Noise: {}, Points per cluster:\".format(len(bincounts) - 1, bincounts[0], ))\n",
    "        print(bincounts[1:])\n",
    "\n",
    "        # scaled values\n",
    "        labels_scaled = dbscan.fit_predict(movie_df_scaled)\n",
    "        bincounts_scaled = np.bincount(labels_scaled + 1)\n",
    "        print(\"-- scaled --\")\n",
    "        print(\"Clusters: {}, Noise: {}, Points per cluster:\".format(len(bincounts_scaled) - 1, bincounts_scaled[0], ))\n",
    "        print(\"{}\\n\".format(bincounts_scaled[1:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain your observations in terms of eps and min_sample effect on the scaled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result for the scaled dataset when adjusting eps and min_sample is:\n",
    "eps: 1, min_sample: 3\n",
    "\n",
    "Note: This still produces a lot of noise\n",
    "\n",
    "Setting eps to 3 makes the area too large so all of the data points are in one cluster. Increasing min_samples causes more points to be labelled as noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain your observations in terms of scaling effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An eps of 2 or 3 with the original data produce the best results. I suspect this is because the One Hot data skews the clusters when everything is scaled between 0 and 1.\n",
    "\n",
    "The best result overall is eps: 3, min_samples: 5 since this gives us a small number of large groups with a low amount of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part B.2 KMeans and Agglomerative Clustering  (3 marks) </h3>\n",
    "In this section, use the scaled dataset and first draw a dendrogram using ward function in scipy. Explian why K= 10 seems like a reasonable choice.\n",
    "Comapre ARI of KMeans and agglomerative using K=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7a5dbcbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "# draw dendrogram\n",
    "linkage_array = ward(movie_df_scaled)\n",
    "plt.figure(figsize=(20, 5))\n",
    "dendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "km = KMeans(n_clusters=10)\n",
    "km_labels = km.fit_predict(movie_df_scaled)\n",
    "\n",
    "ag = AgglomerativeClustering(n_clusters=10)\n",
    "ag_labels = ag.fit_predict(movie_df_scaled)\n",
    "\n",
    "print(\"ARI: {:.2f}\".format(adjusted_rand_score(km_labels, ag_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part C. Feature Selection (6 marks)</h1>\n",
    "<br>\n",
    "\n",
    "In this section, we are going to select the most informative features from the NASA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Part C.1. Feature selection using ANOVA (3 marks)</h2>\n",
    "\n",
    "Steps:\n",
    "    \n",
    "- Read data from the NASA.csv\n",
    "- Using ANOVA select top K features, whith K=[1..10]\n",
    "- Build LogisticRegression models, one with the original data, and one for each K (using a subset of feature)\n",
    "- Compare the accuracies and find the best K (based on the median of 30 runs with random_state=[0 to 30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "#\n",
    "# Read data from the NASA.csv\n",
    "#\n",
    "nasa_path = \"NasaData.csv\"\n",
    "try:\n",
    "    nasa_df = pd.read_csv(nasa_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "\n",
    "# display(nasa_df)\n",
    "\n",
    "#\n",
    "# Using ANOVA select top K features, whith K=[1..10]\n",
    "#\n",
    "for k_val in range(1, 11):\n",
    "    kbest = SelectKBest(k=k_val)\n",
    "\n",
    "\n",
    "#\n",
    "# Build LogisticRegression models, one with the original data, and one for each K (using a subset of feature)\n",
    "#\n",
    "\n",
    "#\n",
    "# Compare the accuracies and find the best K (based on the median of 30 runs with random_state=[0 to 30])\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part C.2. Compare feature selection models (3 marks)</h2>\n",
    "\n",
    "Now apply SelectFromModel and RFE and compare them with SelectKBest, as follows:\n",
    "\n",
    "- Apply the three techniques so that you reduce the features to only 6 features (note that 6 is not necessarily the best K from Part C.1)\n",
    "- Report the prediction scores of a LogisticRegression model on the selected features of each model.\n",
    "- Print the name of features selected by each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part D. Data Tranformation (4 marks)</h1>\n",
    "\n",
    "In this part, you are going to work with a new data set which contains some the features of a house collected over time. \n",
    "The objective of this part is to help improve linear model's predicitons using data transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part D.1 Binning (2 marks)</h2>\n",
    "\n",
    "Our first try is using binning, as follows:\n",
    "\n",
    "- Read from MyHouse.csv (take 'Light' as the data target and the rest of the columns as data features ) \n",
    "- First apply a LinearRegression on the original data to predict the target and report the score of the model on the test set. \n",
    "- Now apply binning on all three columns \n",
    " (for Temperature make 5 Bins -- for Humidity make 10 bins -- and for CO2Bins make 11 bins)\n",
    "- Print your data shape before and after binning.\n",
    "- Now again apply LinearRegression on the new data and report the score again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>CO2</th>\n",
       "      <th>Light</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.700000</td>\n",
       "      <td>26.272000</td>\n",
       "      <td>749.200000</td>\n",
       "      <td>585.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.718000</td>\n",
       "      <td>26.290000</td>\n",
       "      <td>760.400000</td>\n",
       "      <td>578.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.730000</td>\n",
       "      <td>26.230000</td>\n",
       "      <td>769.666667</td>\n",
       "      <td>572.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.722500</td>\n",
       "      <td>26.125000</td>\n",
       "      <td>774.750000</td>\n",
       "      <td>493.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.754000</td>\n",
       "      <td>26.200000</td>\n",
       "      <td>779.000000</td>\n",
       "      <td>488.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.760000</td>\n",
       "      <td>26.260000</td>\n",
       "      <td>790.000000</td>\n",
       "      <td>568.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.730000</td>\n",
       "      <td>26.290000</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>536.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23.754000</td>\n",
       "      <td>26.290000</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>509.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23.754000</td>\n",
       "      <td>26.350000</td>\n",
       "      <td>803.200000</td>\n",
       "      <td>476.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23.736000</td>\n",
       "      <td>26.390000</td>\n",
       "      <td>809.000000</td>\n",
       "      <td>510.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.745000</td>\n",
       "      <td>26.445000</td>\n",
       "      <td>815.250000</td>\n",
       "      <td>481.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23.700000</td>\n",
       "      <td>26.560000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>481.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23.700000</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>475.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23.700000</td>\n",
       "      <td>26.700000</td>\n",
       "      <td>845.333333</td>\n",
       "      <td>469.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>23.700000</td>\n",
       "      <td>26.774000</td>\n",
       "      <td>852.400000</td>\n",
       "      <td>464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23.700000</td>\n",
       "      <td>26.890000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23.700000</td>\n",
       "      <td>26.972500</td>\n",
       "      <td>880.000000</td>\n",
       "      <td>455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>23.600000</td>\n",
       "      <td>26.890000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>454.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>23.640000</td>\n",
       "      <td>26.976000</td>\n",
       "      <td>897.600000</td>\n",
       "      <td>458.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23.650000</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>900.500000</td>\n",
       "      <td>464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23.640000</td>\n",
       "      <td>27.100000</td>\n",
       "      <td>908.800000</td>\n",
       "      <td>473.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23.600000</td>\n",
       "      <td>27.160000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.600000</td>\n",
       "      <td>27.236000</td>\n",
       "      <td>925.200000</td>\n",
       "      <td>498.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.600000</td>\n",
       "      <td>27.290000</td>\n",
       "      <td>929.400000</td>\n",
       "      <td>530.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23.600000</td>\n",
       "      <td>27.330000</td>\n",
       "      <td>936.400000</td>\n",
       "      <td>533.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>23.600000</td>\n",
       "      <td>27.340000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>524.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23.625000</td>\n",
       "      <td>27.392500</td>\n",
       "      <td>961.000000</td>\n",
       "      <td>498.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23.600000</td>\n",
       "      <td>27.390000</td>\n",
       "      <td>963.000000</td>\n",
       "      <td>516.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>23.600000</td>\n",
       "      <td>27.412000</td>\n",
       "      <td>958.600000</td>\n",
       "      <td>501.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23.600000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>965.333333</td>\n",
       "      <td>522.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>23.790000</td>\n",
       "      <td>26.310000</td>\n",
       "      <td>1158.200000</td>\n",
       "      <td>763.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>23.790000</td>\n",
       "      <td>26.315000</td>\n",
       "      <td>1160.250000</td>\n",
       "      <td>758.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>23.890000</td>\n",
       "      <td>26.390000</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>755.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>23.890000</td>\n",
       "      <td>26.365000</td>\n",
       "      <td>1164.750000</td>\n",
       "      <td>758.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>23.890000</td>\n",
       "      <td>26.290000</td>\n",
       "      <td>1163.250000</td>\n",
       "      <td>772.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>23.984286</td>\n",
       "      <td>26.375714</td>\n",
       "      <td>1172.714286</td>\n",
       "      <td>767.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>26.365000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>774.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>24.050000</td>\n",
       "      <td>26.340000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>774.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>24.080000</td>\n",
       "      <td>26.370000</td>\n",
       "      <td>1187.600000</td>\n",
       "      <td>769.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>24.100000</td>\n",
       "      <td>26.566667</td>\n",
       "      <td>1191.333333</td>\n",
       "      <td>758.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>24.100000</td>\n",
       "      <td>26.472500</td>\n",
       "      <td>1213.750000</td>\n",
       "      <td>775.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>24.100000</td>\n",
       "      <td>26.154000</td>\n",
       "      <td>1194.000000</td>\n",
       "      <td>775.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>24.100000</td>\n",
       "      <td>25.958000</td>\n",
       "      <td>1173.400000</td>\n",
       "      <td>783.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>24.100000</td>\n",
       "      <td>25.918000</td>\n",
       "      <td>1160.200000</td>\n",
       "      <td>789.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>24.100000</td>\n",
       "      <td>25.892500</td>\n",
       "      <td>1156.250000</td>\n",
       "      <td>797.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>24.100000</td>\n",
       "      <td>25.832000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>799.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>24.175000</td>\n",
       "      <td>25.840000</td>\n",
       "      <td>1140.500000</td>\n",
       "      <td>782.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2652</th>\n",
       "      <td>24.200000</td>\n",
       "      <td>25.891667</td>\n",
       "      <td>1145.333333</td>\n",
       "      <td>786.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>24.200000</td>\n",
       "      <td>25.890000</td>\n",
       "      <td>1149.250000</td>\n",
       "      <td>794.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>24.200000</td>\n",
       "      <td>25.890000</td>\n",
       "      <td>1152.500000</td>\n",
       "      <td>797.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>24.200000</td>\n",
       "      <td>25.890000</td>\n",
       "      <td>1153.250000</td>\n",
       "      <td>787.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>24.218000</td>\n",
       "      <td>25.912000</td>\n",
       "      <td>1152.400000</td>\n",
       "      <td>805.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>24.260000</td>\n",
       "      <td>25.891667</td>\n",
       "      <td>1146.166667</td>\n",
       "      <td>798.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>24.290000</td>\n",
       "      <td>25.978000</td>\n",
       "      <td>1145.400000</td>\n",
       "      <td>793.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>24.290000</td>\n",
       "      <td>25.852000</td>\n",
       "      <td>1140.800000</td>\n",
       "      <td>801.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>24.290000</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>1150.250000</td>\n",
       "      <td>808.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>24.330000</td>\n",
       "      <td>25.736000</td>\n",
       "      <td>1129.200000</td>\n",
       "      <td>809.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>24.330000</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>1125.800000</td>\n",
       "      <td>817.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663</th>\n",
       "      <td>24.356667</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>1123.000000</td>\n",
       "      <td>813.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>24.408333</td>\n",
       "      <td>25.681667</td>\n",
       "      <td>1124.000000</td>\n",
       "      <td>798.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2665 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Temperature   Humidity          CO2       Light\n",
       "0       23.700000  26.272000   749.200000  585.200000\n",
       "1       23.718000  26.290000   760.400000  578.400000\n",
       "2       23.730000  26.230000   769.666667  572.666667\n",
       "3       23.722500  26.125000   774.750000  493.750000\n",
       "4       23.754000  26.200000   779.000000  488.600000\n",
       "5       23.760000  26.260000   790.000000  568.666667\n",
       "6       23.730000  26.290000   798.000000  536.333333\n",
       "7       23.754000  26.290000   797.000000  509.000000\n",
       "8       23.754000  26.350000   803.200000  476.000000\n",
       "9       23.736000  26.390000   809.000000  510.000000\n",
       "10      23.745000  26.445000   815.250000  481.500000\n",
       "11      23.700000  26.560000   824.000000  481.800000\n",
       "12      23.700000  26.600000   832.000000  475.250000\n",
       "13      23.700000  26.700000   845.333333  469.000000\n",
       "14      23.700000  26.774000   852.400000  464.000000\n",
       "15      23.700000  26.890000   861.000000  464.000000\n",
       "16      23.700000  26.972500   880.000000  455.000000\n",
       "17      23.600000  26.890000   891.000000  454.000000\n",
       "18      23.640000  26.976000   897.600000  458.000000\n",
       "19      23.650000  27.050000   900.500000  464.000000\n",
       "20      23.640000  27.100000   908.800000  473.000000\n",
       "21      23.600000  27.160000   918.000000  464.000000\n",
       "22      23.600000  27.236000   925.200000  498.400000\n",
       "23      23.600000  27.290000   929.400000  530.200000\n",
       "24      23.600000  27.330000   936.400000  533.600000\n",
       "25      23.600000  27.340000   950.000000  524.250000\n",
       "26      23.625000  27.392500   961.000000  498.666667\n",
       "27      23.600000  27.390000   963.000000  516.333333\n",
       "28      23.600000  27.412000   958.600000  501.200000\n",
       "29      23.600000  27.500000   965.333333  522.000000\n",
       "...           ...        ...          ...         ...\n",
       "2635    23.790000  26.310000  1158.200000  763.000000\n",
       "2636    23.790000  26.315000  1160.250000  758.250000\n",
       "2637    23.890000  26.390000  1166.000000  755.500000\n",
       "2638    23.890000  26.365000  1164.750000  758.250000\n",
       "2639    23.890000  26.290000  1163.250000  772.750000\n",
       "2640    23.984286  26.375714  1172.714286  767.000000\n",
       "2641    24.000000  26.365000  1182.000000  774.500000\n",
       "2642    24.050000  26.340000  1182.000000  774.500000\n",
       "2643    24.080000  26.370000  1187.600000  769.800000\n",
       "2644    24.100000  26.566667  1191.333333  758.333333\n",
       "2645    24.100000  26.472500  1213.750000  775.750000\n",
       "2646    24.100000  26.154000  1194.000000  775.000000\n",
       "2647    24.100000  25.958000  1173.400000  783.400000\n",
       "2648    24.100000  25.918000  1160.200000  789.600000\n",
       "2649    24.100000  25.892500  1156.250000  797.500000\n",
       "2650    24.100000  25.832000  1139.000000  799.000000\n",
       "2651    24.175000  25.840000  1140.500000  782.500000\n",
       "2652    24.200000  25.891667  1145.333333  786.166667\n",
       "2653    24.200000  25.890000  1149.250000  794.250000\n",
       "2654    24.200000  25.890000  1152.500000  797.500000\n",
       "2655    24.200000  25.890000  1153.250000  787.000000\n",
       "2656    24.218000  25.912000  1152.400000  805.000000\n",
       "2657    24.260000  25.891667  1146.166667  798.000000\n",
       "2658    24.290000  25.978000  1145.400000  793.000000\n",
       "2659    24.290000  25.852000  1140.800000  801.400000\n",
       "2660    24.290000  25.700000  1150.250000  808.000000\n",
       "2661    24.330000  25.736000  1129.200000  809.800000\n",
       "2662    24.330000  25.700000  1125.800000  817.000000\n",
       "2663    24.356667  25.700000  1123.000000  813.000000\n",
       "2664    24.408333  25.681667  1124.000000  798.000000\n",
       "\n",
       "[2665 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict 'Light' using default Linear Regression\n",
      "--\n",
      "Test set score: 0.69\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#\n",
    "# Read from MyHouse.csv (take 'Light' as the data target and the rest of the columns as data features)\n",
    "#\n",
    "house_path = \"MyHouse.csv\"\n",
    "try:\n",
    "    house_df = pd.read_csv(house_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    \n",
    "display(house_df)\n",
    "\n",
    "#\n",
    "# First apply a LinearRegression on the original data to predict the target and report the score\n",
    "# of the model on the test set.\n",
    "#\n",
    "target_col = \"Light\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    house_df.drop(target_col, axis=1),\n",
    "    house_df[target_col],\n",
    "    test_size=0.25,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# print score\n",
    "print(\"Predict 'Light' using default Linear Regression\\n--\")\n",
    "print(\"Test set score: {:.2f}\\n\\n\".format(lr.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "#\n",
    "# Now apply binning on all three columns (for Temperature make 5 Bins -- for Humidity make 10 bins --\n",
    "# and for CO2Bins make 11 bins)\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Print your data shape before and after binning.\n",
    "#\n",
    "\n",
    "\n",
    "#\n",
    "# Now again apply LinearRegression on the new data and report the score again.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part D.2 Polynomials (2 marks)</h2>\n",
    "\n",
    "To compare polynomials and binning, apply polynomials on all three features. \n",
    "- Use degree=6.\n",
    "- Print your data shape before and after transformation\n",
    "- Apply LinearRegression on the new data and report the score again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part D.2 Compare feature selection models \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
