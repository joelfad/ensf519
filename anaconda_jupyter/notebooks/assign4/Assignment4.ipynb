{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> ENSF 519.01 Applied Data Scince </center></h1>\n",
    "<h2> <center> Assignment 4: Unsupervised learning (25 marks)</center></h2>\n",
    "<h2> <center> Due: March 25, Midnight . To be submitted on D2L Dropbox </center></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment the focus is on using unsupervised learning e.g., clustering and data transformation to get better understanding of the data (exploratory analysis) or predictions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part A. Local vs. Global Prediction (7 marks)</h2>\n",
    "<br><br>\n",
    "In many situations, your training dataset is very large to include as many observations as possible, which is typically a good thing. For instance, a complex prediction model for automated image captioning works best if the learning dataset is massive and rich. However, if you are using a simple model (like a linear regression) for any reason (e.g., speed, interpretability, etc.) one caveat of very large datasets is that the learned models might actually become very far off from your test set.\n",
    "\n",
    "For instance, assume you have to predict online sales for a particular book. But your training set is the entire Amazonâ€™s historical sales records. Obviously, a simple linear model will not work well, trying to fit a line that predicts sales of everything, from books, to grocery, to toys, etc.\n",
    "\n",
    "One simple solution could be trying to train your model only on a portion of the training set that is closer to your interested data. For instance, in the above example, only train on the book records. \n",
    "\n",
    "In this part of HW4, we want to evaluate this idea automatically, by first clustering the training dataset and then fitting the model only on the closets cluster to the test data.\n",
    "\n",
    "\n",
    "So you need to follow these steps: \n",
    "\n",
    "- Read data from OO-DefectPrediction.csv dataset (used in HW3 for defect prediction)\n",
    "- Take 90% of data as train and 10% as test using train_test_split with random_state=0. \n",
    "- Cluster the training set to multiple clusters using Kmeans. (K from 2 to 5, inclusive)\n",
    "- Find the most similar cluster to the test set. To do this predict the data set items and see which cluster they fall in. Then use the majority vote to decide which cluster is closer to the test data (basically the one that is predicted most, as label, for the test data items)\n",
    "- Build a global prediction model the same as the one you built in HW 3 PartB (using original entire train set)\n",
    "- Build a local linear regression model where you use only the closets cluster as your train dataset (all other setups unchanged). \n",
    "- Compare the result (r2_score) between the global and the local model for each K and explain your observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas v0.22.0\n",
    "import pandas as pd\n",
    "\n",
    "# numpy v1.13.3\n",
    "import numpy as np\n",
    "\n",
    "# sklearn v0.19.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# matplotlib v2.1.2\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "#\n",
    "# Read data from OO-DefectPrediction.csv dataset (used in HW3 for defect prediction)\n",
    "#\n",
    "defect_path = \"OO-DefectPrediction.csv\"\n",
    "try:\n",
    "    defect_df = pd.read_csv(defect_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "#\n",
    "# Take 90% of data as train and 10% as test using train_test_split with random_state=0. \n",
    "#\n",
    "target_col = \"bug\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    defect_df.drop(target_col, axis=1),\n",
    "    defect_df[target_col],\n",
    "    test_size=0.10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# display(defect_df)\n",
    "\n",
    "\n",
    "#\n",
    "# Cluster the training set to multiple clusters using Kmeans. (K from 2 to 5, inclusive)\n",
    "#\n",
    "kmeans = dict()\n",
    "for i in range(2, 6):\n",
    "    kmeans[i] = KMeans(n_clusters=i)\n",
    "    kmeans[i].fit(X_train)\n",
    "    print(\"###\")\n",
    "    print(\"Cluster memberships:\\n{}\".format(kmeans[i].labels_))\n",
    "    print(\"---\")\n",
    "    print(\"Predictions:\\n{}\".format(kmeans[i].predict(X_test)))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find the most similar cluster to the test set. To do this predict the data set items and\n",
    "# see which cluster they fall in. Then use the majority vote to decide which cluster is closer\n",
    "# to the test data (basically the one that is predicted most, as label, for the test data items)\n",
    "\n",
    "\n",
    "\n",
    "# Build a global prediction model the same as the one you built in HW 3 PartB (using original entire train set)\n",
    "\n",
    "\n",
    "\n",
    "# Build a local linear regression model where you use only the closest cluster as your train dataset\n",
    "# (all other setups unchanged).\n",
    "\n",
    "\n",
    "\n",
    "# Compare the result (r2_score) between the global and the local model for each K and explain your observations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question: Explain your observation in terms of R^2 of global and local models across different Ks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer:</b> ..................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part B. Analyze Clustering Algorithms (8 marks) </h2>\n",
    "In this part, you will excerise the same type of cluster analysis we tried in class, but this time on a different dataset (Movie Dataset)\n",
    "\n",
    "Preprocessing Steps (2 mark):\n",
    "- Read data from \"movie_metadata.csv\" \n",
    "- Remove duplicate movies (same title), if any exists\n",
    "- Keep only these columns ['director_name', 'actor_1_name', 'budget', 'imdb_score'] as features\n",
    "- Remove any row that has an empty feature, among those we have kept\n",
    "- Use one-hot-encoding in Pandas to transfer categorical data\n",
    "- Finally, scale both 'budget' and 'imdb_score' features using MinMaxScaler \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from copy import copy\n",
    "\n",
    "#\n",
    "# Read data from \"movie_metadata.csv\"\n",
    "#\n",
    "movie_path = \"movie_metadata.csv\"\n",
    "try:\n",
    "    movie_df = pd.read_csv(movie_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "#\n",
    "# Remove duplicate movies (same title), if any exists\n",
    "#\n",
    "movie_df = movie_df.drop_duplicates(\"movie_title\")\n",
    "\n",
    "\n",
    "#\n",
    "# Keep only these columns ['director_name', 'actor_1_name', 'budget', 'imdb_score'] as features\n",
    "#\n",
    "features_to_keep = ['director_name', 'actor_1_name', 'budget', 'imdb_score']\n",
    "movie_df = movie_df[features_to_keep]\n",
    "\n",
    "\n",
    "#\n",
    "# Remove any row that has an empty feature, among those we have kept\n",
    "#\n",
    "movie_df = movie_df.dropna()\n",
    "\n",
    "\n",
    "#\n",
    "# Use one-hot-encoding in Pandas to transfer categorical data\n",
    "#\n",
    "movie_df = pd.get_dummies(movie_df)\n",
    "\n",
    "\n",
    "#\n",
    "# Finally, scale both 'budget' and 'imdb_score' features using MinMaxScaler\n",
    "#\n",
    "features_to_scale = ['budget', 'imdb_score']\n",
    "scaler = MinMaxScaler()\n",
    "movie_df_scaled = copy(movie_df)\n",
    "movie_df_scaled[features_to_scale] = scaler.fit_transform(movie_df_scaled[features_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part B.1 Tune DBSCAN (3 marks) </h3>\n",
    "\n",
    "Now apply a DBSCAN clustering algorithm on the preprocessed data.\n",
    "- First tune DBSCAN using eps : [1,2,3] and min_sample: [3, 5, 7].\n",
    "- For each setup, run DBSCAN twice. Once on the scaled data and once with the original 'budget' and 'imdb_score' values.\n",
    "- print number of clusters, cluster sizes, and number of noises. For both cases, per setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## eps: 1, min_samples: 3 ##\n",
      "-- original --\n",
      "Clusters: 1, Noise: 4416, Points per cluster:\n",
      "[3]\n",
      "-- scaled --\n",
      "Clusters: 42, Noise: 4255, Points per cluster:\n",
      "[10  6  3  3  4 10  3  3  3  4  4  3  7  3  3  4  3  5  4  5  4  3  3  4  3\n",
      "  3  4  4  3  3  4  3  3  5  3  4  3  3  3  3  3  3]\n",
      "\n",
      "## eps: 1, min_samples: 5 ##\n",
      "-- original --\n",
      "Clusters: 0, Noise: 4419, Points per cluster:\n",
      "[]\n",
      "-- scaled --\n",
      "Clusters: 7, Noise: 4371, Points per cluster:\n",
      "[10  6 10  7  5  5  5]\n",
      "\n",
      "## eps: 1, min_samples: 7 ##\n",
      "-- original --\n",
      "Clusters: 0, Noise: 4419, Points per cluster:\n",
      "[]\n",
      "-- scaled --\n",
      "Clusters: 3, Noise: 4392, Points per cluster:\n",
      "[10 10  7]\n",
      "\n",
      "## eps: 2, min_samples: 3 ##\n",
      "-- original --\n",
      "Clusters: 318, Noise: 2366, Points per cluster:\n",
      "[ 88  50  56   3   9  10   5   4   7  65 104   4   4   3   4  39   3   9\n",
      "   3   5   5  24   7   4   6   4   4  88   4   3   6   8   6  59   4   3\n",
      "   4   3   5   5   5   4   4   4   5   4   3   4   3   6   3   4   4   3\n",
      "   6   6   3  13   3  13   3   3   4   6  10   9   5   3   4   6   3   4\n",
      "   5   4  32  15   7  42   5  11   4   6   3  12   5   4   4   4   3   3\n",
      "   3   3   3   4  26   5   5   3  11   9   3  31   3   3   5   3   3  10\n",
      "   3   3   3   4   3   4   4   8   3   6   5   4   4  32   4   4   5   3\n",
      "   3   4   4   4   3   5   4   6   3   3   6   5   3   3   3   4   9   3\n",
      "   3  10   7   3   3   6   3   4   8   6   4   4   3   5   3   6   3   5\n",
      "   3   3   4   4   4  13   3   3   3   3   5   3   3   3   3   6   3   3\n",
      "   3   5   4   3   3   3   3   3   3   4   3   4   8   7   3   6   6   4\n",
      "   3   5   3   4   3   3   5   4   5   4   4   4   3   3   3   4   3   4\n",
      "   3   3   4   7   6   3   5   5   5   3   3   4   3   4   3   4   4   3\n",
      "   3   6   3   4   3   5   4   3   3   3   3   3   3   8   7   4   4   4\n",
      "   3   6   3   4   9   3   3   3   3   4   3   8   5   5   3   3   3   4\n",
      "   6   3   3   3   3   4   3   3   4   3   6   5   3   3   3   3   3   5\n",
      "   3   3   7   4   4   4   3   3   3   3   4   3   3   3   3   3   3   3\n",
      "   5   4   3   3   3   3   3   3   3   3   3   3]\n",
      "-- scaled --\n",
      "Clusters: 6, Noise: 389, Points per cluster:\n",
      "[4014    3    4    3    3    3]\n",
      "\n",
      "## eps: 2, min_samples: 5 ##\n",
      "-- original --\n",
      "Clusters: 86, Noise: 3303, Points per cluster:\n",
      "[70 42 55  9  6  5  7 96 64 39  9  5 22  6 59  6  6  6  5  5  5  6 16 50  5\n",
      " 31 14 20  6 12 22  5  5 11 22 11  5 11  8  8  6 29  5  5 10 10  9  5  6  5\n",
      "  5  8 10  9  5  5 10  6  9  5  6  6  7  5  5  7  6  5  5 16  5  5  6  7  9\n",
      "  7  6  5  5  5  5  6  5  6  4  5]\n",
      "-- scaled --\n",
      "Clusters: 1, Noise: 503, Points per cluster:\n",
      "[3916]\n",
      "\n",
      "## eps: 2, min_samples: 7 ##\n",
      "-- original --\n",
      "Clusters: 39, Noise: 3805, Points per cluster:\n",
      "[32 45  7 84 29 21 21 31 16 16  9  7 18  7 12 10  9  8  7 26 24  9  8 24 10\n",
      " 15 11  7 14  9 11  7  7  7  7  7  8  7  7]\n",
      "-- scaled --\n",
      "Clusters: 1, Noise: 632, Points per cluster:\n",
      "[3787]\n",
      "\n",
      "## eps: 3, min_samples: 3 ##\n",
      "-- original --\n",
      "Clusters: 176, Noise: 325, Points per cluster:\n",
      "[136 117 129  53  38 139  67  64  75  21  91  45 138  45  21  77  69  45\n",
      " 168  15  99  17  14   4   6  37   6 107  34   6  13  41  49  55  27  79\n",
      "  19  51  10  41  19  41   3  13 134  40  17   8  90  59   3   9  20  73\n",
      "   8  25  15  17   7  21  47  50  25   5  45  11   3  60  11   6   6  16\n",
      "   5  56   6   4   3   9   6   4   8  22  31   3   3  18   5   3  25  41\n",
      "  15   6   7  27  10  15  28   7   4   5   8  17   7  54  12   3   6  12\n",
      "   5   5   6   5   5  21   3   9   8   7  19   6   4   5   4   4   3   3\n",
      "   7  30   9   3   5  10   8  12   9   3  12   5   7   5   4   3   5   6\n",
      "   8   4  12   5   8   5   3   4   4   4   5   4   3   5  10   6   7   3\n",
      "   8   4   3   5   5   6   5   5   3   3   3   3   3   3]\n",
      "-- scaled --\n",
      "Clusters: 1, Noise: 0, Points per cluster:\n",
      "[4419]\n",
      "\n",
      "## eps: 3, min_samples: 5 ##\n",
      "-- original --\n",
      "Clusters: 138, Noise: 456, Points per cluster:\n",
      "[136 117 129  53  38 139  67  64  75  21  91  45 138  45  21  77  69  45\n",
      " 168  15  99  17  14   6  37   6 107  34   6  13  41  49  55  27  79  19\n",
      "  51  10  41  19  41  13 134  40  17   8  90  59   9  20  73   8  25  15\n",
      "  17   7  21  47  50  25  45  11  60  11   6   6  16   5  56   6   9   8\n",
      "  22  31  18   5   5  25  41  15   6   7  27  10  15  28   7   5   8  17\n",
      "   7  54  12   5   5   5   6   5   5  21   8   8   7  19   6   7  30   9\n",
      "   5  10   8  12   9  12  12   5   7   5   6   8  12   5   8   5   5   5\n",
      "  10   6   7   8   5   5   5   6   5   5   5   5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#\n",
    "# First tune DBSCAN using eps: [1,2,3] and min_sample: [3, 5, 7].\n",
    "#\n",
    "for e in range(1, 4):\n",
    "    for s in range(3, 9, 2):\n",
    "        dbscan = DBSCAN(eps=e, min_samples=s)\n",
    "\n",
    "        # original values\n",
    "        labels = dbscan.fit_predict(movie_df)\n",
    "        bincounts = np.bincount(labels + 1)\n",
    "        print(\"## eps: {}, min_samples: {} ##\\n-- original --\".format(e, s))\n",
    "        print(\"Clusters: {}, Noise: {}, Points per cluster:\".format(len(bincounts) - 1, bincounts[0], ))\n",
    "        print(bincounts[1:])\n",
    "\n",
    "        # scaled values\n",
    "        labels_scaled = dbscan.fit_predict(movie_df_scaled)\n",
    "        bincounts_scaled = np.bincount(labels_scaled + 1)\n",
    "        print(\"-- scaled --\")\n",
    "        print(\"Clusters: {}, Noise: {}, Points per cluster:\".format(len(bincounts_scaled) - 1, bincounts_scaled[0], ))\n",
    "        print(\"{}\\n\".format(bincounts_scaled[1:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain your observations in terms of eps and min_sample effect on the scaled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer:</b> ..................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain your observations in terms of scaling effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Your Answer:</b> ..................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part B.2 KMeans and Agglomerative Clustering  (3 marks) </h3>\n",
    "In this section, use the scaled dataset and first draw a dendrogram using ward function in scipy. Explian why K= 10 seems like a reasonable choice.\n",
    "Comapre ARI of KMeans and agglomerative using K=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from \n",
    "km = KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part C. Feature Selection (6 marks)</h1>\n",
    "<br>\n",
    "\n",
    "In this section, we are going to select the most informative features from the NASA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Part C.1. Feature selection using ANOVA (3 marks)</h2>\n",
    "\n",
    "Steps:\n",
    "    \n",
    "- Read data from the NASA.csv\n",
    "- Using ANOVA select top K features, whith K=[1..10]\n",
    "- Build LogisticRegression models, one with the original data, and one for each K (using a subset of feature)\n",
    "- Compare the accuracies and find the best K (based on the median of 30 runs with random_state=[0 to 30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Read data from the NASA.csv\n",
    "#\n",
    "nasa_path = \"NasaData.csv\"\n",
    "try:\n",
    "    nasa_df = pd.read_csv(nasa_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "\n",
    "# display(nasa_df)\n",
    "\n",
    "#\n",
    "# Using ANOVA select top K features, whith K=[1..10]\n",
    "#\n",
    "\n",
    "#\n",
    "# Build LogisticRegression models, one with the original data, and one for each K (using a subset of feature)\n",
    "#\n",
    "\n",
    "#\n",
    "# Compare the accuracies and find the best K (based on the median of 30 runs with random_state=[0 to 30])\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part C.2. Compare feature selection models (3 marks)</h2>\n",
    "\n",
    "Now apply SelectFromModel and RFE and compare them with SelectKBest, as follows:\n",
    "\n",
    "- Apply the three techniques so that you reduce the features to only 6 features (note that 6 is not necessarily the best K from Part C.1)\n",
    "- Report the prediction scores of a LogisticRegression model on the selected features of each model.\n",
    "- Print the name of features selected by each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part C.2 Compare feature selection models \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part D. Data Tranformation (4 marks)</h1>\n",
    "\n",
    "In this part, you are going to work with a new data set which contains some the features of a house collected over time. \n",
    "The objective of this part is to help improve linear model's predicitons using data transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part D.1 Binning (2 marks)</h2>\n",
    "\n",
    "Our first try is using binning, as follows:\n",
    "\n",
    "- Read from MyHouse.csv (take 'Light' as the data target and the rest of the columns as data features ) \n",
    "- First apply a LinearRegression on the original data to predict the target and report the score of the model on the test set. \n",
    "- Now apply binning on all three columns \n",
    " (for Temperature make 5 Bins -- for Humidity make 10 bins -- and for CO2Bins make 11 bins)\n",
    "- Print your data shape before and after binning.\n",
    "- Now again apply LinearRegression on the new data and report the score again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#\n",
    "# Read from MyHouse.csv (take 'Light' as the data target and the rest of the columns as data features)\n",
    "#\n",
    "house_path = \"MyHouse.csv\"\n",
    "try:\n",
    "    house_df = pd.read_csv(house_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    \n",
    "display(house_df)\n",
    "\n",
    "#\n",
    "# First apply a LinearRegression on the original data to predict the target and report the score\n",
    "# of the model on the test set.\n",
    "#\n",
    "target_col = \"Light\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    house_df.drop(target_col, axis=1),\n",
    "    house_df[target_col],\n",
    "    test_size=0.25,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# print score\n",
    "print(\"Predict 'Light' using default Linear Regression\\n--\")\n",
    "print(\"Test set score: {:.2f}\\n\\n\".format(lr.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "#\n",
    "# Now apply binning on all three columns (for Temperature make 5 Bins -- for Humidity make 10 bins --\n",
    "# and for CO2Bins make 11 bins)\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Print your data shape before and after binning.\n",
    "#\n",
    "\n",
    "\n",
    "#\n",
    "# Now again apply LinearRegression on the new data and report the score again.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part D.2 Polynomials (2 marks)</h2>\n",
    "\n",
    "To compare polynomials and binning, apply polynomials on all three features. \n",
    "- Use degree=6.\n",
    "- Print your data shape before and after transformation\n",
    "- Apply LinearRegression on the new data and report the score again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part D.2 Compare feature selection models \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
